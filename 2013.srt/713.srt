
1
00:00:00.506 --> 00:00:09.566
[ Silence ]

2
00:00:10.066 --> 00:00:10.866
>> Good afternoon.

3
00:00:11.116 --> 00:00:13.026
Welcome to the Accelerate
Framework Session.

4
00:00:13.726 --> 00:00:14.846
My name's Jeff Belcher.

5
00:00:15.546 --> 00:00:17.536
I'm an engineer in the
Vector and Numerics Group.

6
00:00:18.776 --> 00:00:21.076
Today I want to start off
with a pretty common scenario.

7
00:00:22.036 --> 00:00:24.476
Imagine you've got a great
idea for an application,

8
00:00:24.666 --> 00:00:26.936
and that application
has a computationally

9
00:00:26.936 --> 00:00:27.956
intensive component.

10
00:00:29.516 --> 00:00:31.506
You look around and you
find an open source solution

11
00:00:31.506 --> 00:00:34.546
to the problem, you bring
it into your application,

12
00:00:34.676 --> 00:00:37.916
you test it, and you find
the graph's too slow,

13
00:00:39.076 --> 00:00:40.516
or maybe it's a battery drain.

14
00:00:41.966 --> 00:00:44.526
At this point you're forced to
spend the next several hours

15
00:00:44.526 --> 00:00:47.386
or maybe days, profiling
and optimizing that code

16
00:00:48.446 --> 00:00:50.406
to get the performance to
where you need it to be.

17
00:00:51.796 --> 00:00:53.496
We don't think that's right.

18
00:00:54.026 --> 00:00:55.706
The goal of the Accelerate
Framework is

19
00:00:55.736 --> 00:00:56.976
to solve this problem.

20
00:00:58.496 --> 00:01:01.116
The Accelerate Framework is
a collection of functions

21
00:00:58.496 --> 00:01:01.116
The Accelerate Framework is
a collection of functions

22
00:01:01.116 --> 00:01:04.286
of commonly used computationally
intensive operations.

23
00:01:04.836 --> 00:01:08.436
The Accelerate Framework is
designed to be high performance

24
00:01:08.706 --> 00:01:12.366
and deliver great
energy savings for all

25
00:01:12.366 --> 00:01:13.946
of these APIs that
are available.

26
00:01:14.556 --> 00:01:17.236
When you adopt the Accelerate
Framework you're going

27
00:01:17.236 --> 00:01:20.126
to get great performance and
amazing energy characteristics

28
00:01:20.126 --> 00:01:22.576
from the smallest
iPhone all the way

29
00:01:22.576 --> 00:01:24.216
up through the biggest Mac Pro

30
00:01:24.276 --> 00:01:27.226
without changing a single
line of code on your end.

31
00:01:28.286 --> 00:01:30.546
Let's dive into the details
of the Accelerate Framework

32
00:01:30.546 --> 00:01:33.546
and see how it can help you
make a really great app.

33
00:01:35.696 --> 00:01:37.576
So what is the Accelerate
Framework?

34
00:01:39.156 --> 00:01:41.146
When you think Accelerate
Framework there's a few things

35
00:01:41.146 --> 00:01:42.106
that I want you to remember.

36
00:01:42.766 --> 00:01:45.226
First, easy access to
a lot of functionality.

37
00:01:46.536 --> 00:01:48.636
There's more than
2,000 APIs available

38
00:01:48.636 --> 00:01:49.716
on the Accelerate Framework.

39
00:01:50.346 --> 00:01:52.856
Throughout the rest of
the talk we'll break this

40
00:01:52.886 --> 00:01:54.876
down into four easy-to-remember
categories

41
00:01:54.876 --> 00:01:56.956
and show you what
exactly is available.

42
00:01:57.506 --> 00:01:58.486
Think accurate.

43
00:01:58.606 --> 00:02:03.276
We spent a lot of time testing
so that you don't have to.

44
00:01:58.606 --> 00:02:03.276
We spent a lot of time testing
so that you don't have to.

45
00:02:04.196 --> 00:02:06.886
The big one is fast
with low energy usage.

46
00:02:07.646 --> 00:02:09.026
You guys really pushed
the limits

47
00:02:09.026 --> 00:02:12.116
of the hardware available today
with your great applications.

48
00:02:12.726 --> 00:02:14.216
When you use the Accelerate
Framework you're going

49
00:02:14.216 --> 00:02:16.366
to get great performance,
and that's going to come

50
00:02:16.366 --> 00:02:18.506
with amazing energy
characteristics.

51
00:02:19.056 --> 00:02:24.256
The best part for you is it
works great on both OS X and iOS

52
00:02:24.876 --> 00:02:28.426
and it's optimized for all
generations of hardware,

53
00:02:29.066 --> 00:02:30.896
so when new hardware
comes out you're not going

54
00:02:30.896 --> 00:02:35.666
to have to revisit your code.

55
00:02:35.866 --> 00:02:38.106
So I mentioned that there's
a lot of functionality

56
00:02:38.266 --> 00:02:41.106
and the Accelerate Framework
is geared toward commonly used

57
00:02:41.226 --> 00:02:42.856
computationally intensive
operations,

58
00:02:43.166 --> 00:02:44.516
but what exactly is available?

59
00:02:45.246 --> 00:02:47.516
We break it down into
these four categories.

60
00:02:48.446 --> 00:02:52.106
First we've got image
processing, with vImage,

61
00:02:53.536 --> 00:02:57.786
we've got digital signal
processing and VVSP,

62
00:02:57.976 --> 00:03:01.556
transcendental math functions
and vForce and vMathLive,

63
00:02:57.976 --> 00:03:01.556
transcendental math functions
and vForce and vMathLive,

64
00:03:03.106 --> 00:03:06.876
and finally, linear
algebra in LAPACK and BLAS.

65
00:03:07.456 --> 00:03:11.726
At the end of this talk
there's a few points

66
00:03:11.726 --> 00:03:12.966
that I want you to
come away with.

67
00:03:13.906 --> 00:03:16.266
The first of these is how the
Accelerate Framework can help

68
00:03:16.266 --> 00:03:18.146
you create a really
great application.

69
00:03:19.106 --> 00:03:20.706
I'm going to show
you some examples

70
00:03:21.276 --> 00:03:23.336
of real world performance
and energy savings

71
00:03:23.336 --> 00:03:25.026
that you can expect
when you utilize the

72
00:03:25.026 --> 00:03:25.876
Accelerate Framework.

73
00:03:26.446 --> 00:03:29.666
I want you to have an
idea of areas of your code

74
00:03:29.666 --> 00:03:34.216
that are likely to benefit
from the Accelerate Framework,

75
00:03:34.766 --> 00:03:37.736
and finally, how to use
the Accelerate Framework.

76
00:03:37.986 --> 00:03:39.816
So this is going to
range from linking

77
00:03:39.816 --> 00:03:43.066
against the Accelerate Framework
up through some tips and tricks

78
00:03:43.066 --> 00:03:45.056
that can really allow
you to get the most

79
00:03:45.056 --> 00:03:46.426
out of the Accelerate Framework.

80
00:03:46.426 --> 00:03:51.736
I want to move now to why the
Accelerate Framework is fast.

81
00:03:53.086 --> 00:03:55.546
Understanding why the Accelerate
Framework is fast can help

82
00:03:55.546 --> 00:03:58.296
in understanding when and why
to use the Accelerate Framework.

83
00:03:58.296 --> 00:04:03.366
One of the big reasons the
Accelerate Framework is fast is

84
00:03:58.296 --> 00:04:03.366
One of the big reasons the
Accelerate Framework is fast is

85
00:04:03.366 --> 00:04:05.166
we utilize SIMD instructions.

86
00:04:06.576 --> 00:04:08.656
This is Single Instruction
Multiple Data.

87
00:04:08.656 --> 00:04:12.496
For those of you
unfamiliar, if we're trying

88
00:04:12.496 --> 00:04:15.856
to for example add 2 arrays
together, there are instructions

89
00:04:15.856 --> 00:04:17.216
on current hardware
that allow us

90
00:04:17.216 --> 00:04:19.255
to add multiple elements
simultaneously.

91
00:04:19.886 --> 00:04:23.196
For those of you more
familiar with SIMD operations,

92
00:04:23.736 --> 00:04:25.796
on Intel this means
we're taking advantage

93
00:04:25.796 --> 00:04:28.736
of SSE, AVX, and now AVX2.

94
00:04:30.186 --> 00:04:34.196
On ARM we're taking
advantage of NEON.

95
00:04:34.366 --> 00:04:35.816
Utilizing SIMD instructions

96
00:04:35.816 --> 00:04:38.166
in certain situations can
have significant energy

97
00:04:38.166 --> 00:04:39.346
and performance savings.

98
00:04:40.606 --> 00:04:45.496
We also spend a lot of time
matching the microarchitecture

99
00:04:45.706 --> 00:04:47.486
for the complete
Apple hardware lineup.

100
00:04:48.386 --> 00:04:51.246
This includes optimizations
like instruction selection

101
00:04:51.246 --> 00:04:52.436
and instruction scheduling,

102
00:04:52.486 --> 00:04:56.016
as well as software
pipelining and loop unrolling.

103
00:04:57.516 --> 00:05:00.256
So I bring these up because
it requires a certain amount

104
00:04:57.516 --> 00:05:00.256
So I bring these up because
it requires a certain amount

105
00:05:00.256 --> 00:05:01.666
of data before optimizations

106
00:05:01.666 --> 00:05:03.396
like loop unrolling
become beneficial,

107
00:05:03.916 --> 00:05:05.116
so it helps to understand

108
00:05:05.346 --> 00:05:07.546
that this is sometimes
happening behind the scenes

109
00:05:07.546 --> 00:05:08.576
in the Accelerated Framework.

110
00:05:09.316 --> 00:05:12.746
The last reason the
Accelerated Framework is fast is

111
00:05:12.746 --> 00:05:15.316
because it's multithreaded
using GCD.

112
00:05:16.326 --> 00:05:18.106
When it's appropriate we're
going to take advantage

113
00:05:18.106 --> 00:05:19.416
of all the cores available.

114
00:05:20.676 --> 00:05:25.806
So I wanted to talk
about why it's fast

115
00:05:25.806 --> 00:05:28.466
so that you have an
understanding of where some

116
00:05:28.466 --> 00:05:30.056
of the tips for successful use

117
00:05:30.056 --> 00:05:31.576
of the Accelerate
Framework come from.

118
00:05:32.116 --> 00:05:35.276
The first tip is
preparation of your data.

119
00:05:35.276 --> 00:05:38.416
When you prepare your
data there's a few things

120
00:05:38.416 --> 00:05:39.466
that I want you to remember.

121
00:05:40.426 --> 00:05:42.756
The first is if you can
make your data contiguous.

122
00:05:43.426 --> 00:05:45.466
This means that if
you're creating an array,

123
00:05:45.766 --> 00:05:46.726
you want to make that array

124
00:05:46.726 --> 00:05:48.566
such that the elements
are contiguous.

125
00:05:49.116 --> 00:05:52.466
If you're allocating or
have control over the layout

126
00:05:52.466 --> 00:05:55.126
of that buffer and memory, if
you can align the beginning

127
00:05:55.126 --> 00:05:57.076
of that buffer to
16-byte boundary,

128
00:05:57.076 --> 00:05:58.196
that's going to be ideal.

129
00:05:58.786 --> 00:06:01.916
With the Accelerate
Framework we always strive

130
00:05:58.786 --> 00:06:01.916
With the Accelerate
Framework we always strive

131
00:06:01.916 --> 00:06:03.396
to deliver the greatest
performance,

132
00:06:03.426 --> 00:06:05.606
but if you can meet
these recommendations,

133
00:06:06.236 --> 00:06:09.066
in certain situations we
can algorithmically exploit

134
00:06:09.066 --> 00:06:11.216
that to give you
slightly more performance.

135
00:06:11.606 --> 00:06:16.556
The next tip is to
understand the problem size.

136
00:06:18.756 --> 00:06:21.476
Any function call has a
cost associated with it.

137
00:06:22.436 --> 00:06:24.846
The Accelerate Framework
is not immune to this.

138
00:06:26.216 --> 00:06:27.836
On the previous slide
we also saw

139
00:06:27.836 --> 00:06:29.686
that in certain situations
optimizations

140
00:06:29.686 --> 00:06:31.456
like loop unrolling are used.

141
00:06:31.966 --> 00:06:34.596
What this means for you is

142
00:06:34.596 --> 00:06:37.456
that when you're
using really small --

143
00:06:37.456 --> 00:06:38.916
when you're using the
Accelerate Framework

144
00:06:38.916 --> 00:06:40.306
with really small datasets,

145
00:06:40.806 --> 00:06:43.246
it may not deliver
the best performance.

146
00:06:44.606 --> 00:06:45.716
There's not a problem size

147
00:06:45.716 --> 00:06:47.766
that I can say don't use
the Accelerate Framework

148
00:06:47.766 --> 00:06:49.966
for something that's
small; it's going to depend

149
00:06:49.966 --> 00:06:51.406
on the operation
you're performing.

150
00:06:52.186 --> 00:06:54.916
For example, if you're scaling a
vector it might be on the order

151
00:06:54.916 --> 00:06:56.916
of 100 elements; whereas

152
00:06:56.916 --> 00:06:59.286
if you have a more complicated
operation for example,

153
00:06:59.286 --> 00:07:02.826
Matrix Multiply, it could
be as small as 8 elements.

154
00:06:59.286 --> 00:07:02.826
Matrix Multiply, it could
be as small as 8 elements.

155
00:07:04.086 --> 00:07:06.196
The best thing you can
do here is to experiment.

156
00:07:07.116 --> 00:07:08.676
The Accelerate Framework
is always going

157
00:07:08.676 --> 00:07:10.196
to deliver the great
functionality,

158
00:07:10.626 --> 00:07:13.066
just for these smaller
problem sizes it may not be the

159
00:07:13.066 --> 00:07:13.986
best performance.

160
00:07:13.986 --> 00:07:20.596
The last tip for successful
use is to do setup once

161
00:07:20.596 --> 00:07:21.956
and destroy once at the end.

162
00:07:23.066 --> 00:07:25.376
There's a handful of operations
in the Accelerate Framework

163
00:07:25.376 --> 00:07:26.896
that require a setup structure.

164
00:07:28.116 --> 00:07:30.096
Creating this setup
structure can be costly

165
00:07:30.096 --> 00:07:30.986
and time-consuming.

166
00:07:31.496 --> 00:07:34.476
These setup structures
are designed

167
00:07:34.476 --> 00:07:38.836
to be used multiple times, so if
you find yourself in a situation

168
00:07:38.836 --> 00:07:42.576
where you need to do these
setups, create the setup,

169
00:07:42.746 --> 00:07:45.246
do all of the computation that
you want to do with that setup,

170
00:07:45.316 --> 00:07:48.696
and then destroy
once at the end.

171
00:07:48.696 --> 00:07:51.116
Throughout the rest of the talk
we'll see some examples of this

172
00:07:51.116 --> 00:07:52.406
and it will become more clear.

173
00:07:53.066 --> 00:07:57.726
Now I want to move on to using
the Accelerate Framework.

174
00:07:58.376 --> 00:08:00.786
For those of you brand new
to the Accelerate Framework,

175
00:07:58.376 --> 00:08:00.786
For those of you brand new
to the Accelerate Framework,

176
00:08:01.046 --> 00:08:03.896
including it is just like
including any other framework.

177
00:08:04.586 --> 00:08:08.946
Here we have a typical Xcode
project, and we're just going

178
00:08:08.946 --> 00:08:10.706
to navigate to the build phases.

179
00:08:11.876 --> 00:08:13.976
In the build phases we're
going to find the link

180
00:08:13.976 --> 00:08:16.056
with the library
section and we're going

181
00:08:16.056 --> 00:08:17.106
to find the Plus button.

182
00:08:18.276 --> 00:08:20.816
This brings up the list
of available frameworks.

183
00:08:21.396 --> 00:08:24.996
The Accelerate Framework's
right at the top,

184
00:08:24.996 --> 00:08:26.406
we'll just select
it and click Add.

185
00:08:27.276 --> 00:08:32.336
And then we can be sure that the
Accelerate Framework is included

186
00:08:32.336 --> 00:08:33.926
in our project because
it's going to show

187
00:08:33.926 --> 00:08:35.566
up in this link the
Library section.

188
00:08:35.976 --> 00:08:40.385
The only other step to using
the Accelerate Framework is

189
00:08:40.416 --> 00:08:41.446
to include the headers.

190
00:08:41.936 --> 00:08:48.826
This is accelerate/accelerate.h.
That's all it takes

191
00:08:48.826 --> 00:08:50.346
to use the Accelerate Framework.

192
00:08:50.966 --> 00:08:53.276
Linking from the Command
line is just as easy.

193
00:08:53.736 --> 00:08:57.396
In your link step simply
include -framework accelerate.

194
00:08:57.396 --> 00:09:03.256
So now I want to dive into the
details of what's available

195
00:08:57.396 --> 00:09:03.256
So now I want to dive into the
details of what's available

196
00:09:03.256 --> 00:09:04.316
in the Accelerate Framework.

197
00:09:05.176 --> 00:09:06.986
I mentioned there's
over 2,000 APIs

198
00:09:06.986 --> 00:09:10.056
and we've got these four
categories so we'll start

199
00:09:10.056 --> 00:09:11.356
to step through these now.

200
00:09:12.346 --> 00:09:14.076
And we'll begin with
image processing.

201
00:09:16.346 --> 00:09:18.416
For image processing
we have vImage,

202
00:09:18.746 --> 00:09:20.946
our vectorized image
processing library.

203
00:09:21.436 --> 00:09:25.866
There's a lot of
functionality in vImage,

204
00:09:26.066 --> 00:09:28.686
and rather than just list it
I put together a short video

205
00:09:28.686 --> 00:09:30.656
to show you some of the
features that are available.

206
00:09:31.246 --> 00:09:33.916
We've got alpha blending

207
00:09:33.916 --> 00:09:37.976
and alpha compositing,
dilation, erosion.

208
00:09:38.816 --> 00:09:41.706
You can create Sobel filters
for form edge detection,

209
00:09:42.826 --> 00:09:46.916
various types of CONVLs
to perform blur, deblur,

210
00:09:47.446 --> 00:09:52.896
or multi-kernel CONVLs,
MaxFilters, MinFilters,

211
00:09:54.416 --> 00:09:59.646
color transformations,
warps and Shears.

212
00:10:00.406 --> 00:10:04.536
So this is just some of
what you'll find in vImage.

213
00:10:05.046 --> 00:10:09.346
We also have some great
additions and improvements

214
00:10:09.766 --> 00:10:13.116
in both iOS 7 and OS X.

215
00:10:13.976 --> 00:10:16.196
First we have improved
conversion support.

216
00:10:17.406 --> 00:10:20.476
Conversions are operations
like converting between planar

217
00:10:20.476 --> 00:10:24.656
and chunky data or changing
between a pixel component type,

218
00:10:24.656 --> 00:10:28.226
so an 8-bit image format
to a 16-bit image format

219
00:10:28.226 --> 00:10:33.556
or a floating point image
format, just to name a few.

220
00:10:33.776 --> 00:10:36.716
We also introduced vImage
buffer creation utilities,

221
00:10:37.826 --> 00:10:39.966
so in the tips I talked
about how important it is

222
00:10:39.966 --> 00:10:43.306
to create a buffer,
getting the alignment right

223
00:10:43.306 --> 00:10:46.246
and getting everything
contiguous, so to take some

224
00:10:46.246 --> 00:10:48.256
of the guesswork out
of that for vImage,

225
00:10:49.056 --> 00:10:50.356
we introduced the utilities

226
00:10:50.356 --> 00:10:53.166
where you can just specify
the size of the image,

227
00:10:53.696 --> 00:10:58.436
and this function will create
the appropriately sized buffer

228
00:10:58.636 --> 00:11:00.556
to deliver the maximum
performance.

229
00:10:58.636 --> 00:11:00.556
to deliver the maximum
performance.

230
00:11:03.696 --> 00:11:07.046
We also introduced
resampling of 16-bit images,

231
00:11:08.246 --> 00:11:11.346
so all the operations like Warp
and Shear that were available

232
00:11:11.346 --> 00:11:14.486
for 8-bit and floating point
image formats are now available

233
00:11:14.486 --> 00:11:16.386
for 16-bit image
formats as well.

234
00:11:16.996 --> 00:11:23.216
The last addition is streamlined
core graphics interoperability.

235
00:11:23.506 --> 00:11:26.476
This is a big one, and I
want to dive into the details

236
00:11:26.476 --> 00:11:27.806
of this with an example.

237
00:11:28.466 --> 00:11:31.616
So we got the question a lot.

238
00:11:31.616 --> 00:11:34.826
How do I use vImage
with my CGImage ref?

239
00:11:35.756 --> 00:11:38.276
To solve this problem
we introduced two new

240
00:11:38.276 --> 00:11:39.536
utility functions.

241
00:11:40.096 --> 00:11:43.656
To go from CGImage
ref to vImage buffer,

242
00:11:43.656 --> 00:11:47.416
we introduced a utility function
vImage buffer and with CGImage

243
00:11:48.466 --> 00:11:50.166
and for the reverse direction,

244
00:11:50.166 --> 00:11:53.206
we introduced vImage
create CGImage from buffer.

245
00:11:53.846 --> 00:11:56.746
Let's take a look at
an example of this,

246
00:11:56.856 --> 00:11:58.856
and see just how
easy it is to use.

247
00:11:59.746 --> 00:12:03.646
So here we're going
to look at how to go

248
00:11:59.746 --> 00:12:03.646
So here we're going
to look at how to go

249
00:12:03.646 --> 00:12:06.646
from a CGImage ref
to a vImage buffer.

250
00:12:07.816 --> 00:12:09.636
As always, we're going to begin

251
00:12:09.636 --> 00:12:11.576
by including the
Accelerate Framework header

252
00:12:12.136 --> 00:12:16.116
and then we're going to
create an openImage ref.

253
00:12:16.966 --> 00:12:19.136
I'm not going to go through
the details of this here.

254
00:12:19.136 --> 00:12:21.496
There's a lot of documentation
and examples of this,

255
00:12:22.136 --> 00:12:26.196
but assume after this line that
we have our CGImage ref open.

256
00:12:26.196 --> 00:12:28.636
The first step that we're going

257
00:12:28.636 --> 00:12:31.046
to do then is specify
the image format.

258
00:12:31.606 --> 00:12:35.986
This image format describes
the format of the vImage buffer

259
00:12:35.986 --> 00:12:37.646
that we want to create.

260
00:12:38.536 --> 00:12:42.156
We've introduced the
vImage/CGImage format structure.

261
00:12:43.526 --> 00:12:45.926
You'll find several elements
in here; for example,

262
00:12:45.926 --> 00:12:49.256
bits per component,
bits per pixel,

263
00:12:49.506 --> 00:12:56.586
information about the color
and bitmap info to name a few.

264
00:12:56.796 --> 00:13:00.576
This descriptor is
describing an ARGB 8-bit image.

265
00:12:56.796 --> 00:13:00.576
This descriptor is
describing an ARGB 8-bit image.

266
00:13:01.876 --> 00:13:03.816
We see that the first entry

267
00:13:03.886 --> 00:13:07.346
in this structure is
bits per component of 8,

268
00:13:07.686 --> 00:13:10.176
so each component in the
picture is going to be 8 bits.

269
00:13:10.976 --> 00:13:12.726
The bits per pixel is 32,

270
00:13:12.726 --> 00:13:14.406
so there's going
to be 4 components.

271
00:13:16.516 --> 00:13:18.306
Color space, we pass null.

272
00:13:18.766 --> 00:13:20.506
When we pass null this
means that we're going

273
00:13:20.506 --> 00:13:22.676
to get a default
RBG color space,

274
00:13:22.826 --> 00:13:24.766
so we have 3 color components.

275
00:13:25.686 --> 00:13:29.926
And then in the bitmap info,
we have kCGImage alpha first.

276
00:13:30.366 --> 00:13:32.396
This means we have a
single alpha component

277
00:13:32.706 --> 00:13:33.886
and it's the first component.

278
00:13:34.406 --> 00:13:37.356
So this describes our
8-bit ARGB image format.

279
00:13:37.926 --> 00:13:43.836
With this format we're going to
call vImage buffer with CGImage.

280
00:13:45.546 --> 00:13:48.306
The first argument is the
input buffer that we want

281
00:13:48.306 --> 00:13:50.626
to create from our CGImage ref.

282
00:13:51.886 --> 00:13:53.546
The second argument
is the reference

283
00:13:53.546 --> 00:13:55.736
to that format description
that we just created.

284
00:13:56.336 --> 00:13:59.636
The third argument is
unused in this case.

285
00:13:59.636 --> 00:14:01.706
This is information
about background color.

286
00:13:59.636 --> 00:14:01.706
This is information
about background color.

287
00:14:02.706 --> 00:14:05.916
In certain conversions when
alpha channels are involved,

288
00:14:05.916 --> 00:14:07.966
it may be necessary
to provide information

289
00:14:07.966 --> 00:14:08.976
about a background color.

290
00:14:09.576 --> 00:14:14.416
The next argument is NImage --

291
00:14:14.416 --> 00:14:16.566
this is our CGImage ref
that we want to convert

292
00:14:16.566 --> 00:14:20.196
to the vImage buffer, and
finally any additional flags.

293
00:14:20.346 --> 00:14:24.226
In this case we don't have any
so we pass kV image no flags.

294
00:14:26.966 --> 00:14:30.046
Upon successful return
of this function,

295
00:14:30.046 --> 00:14:32.406
we've allocated a
new vImage buffer.

296
00:14:32.656 --> 00:14:37.026
It contains the image format,
the image and the format

297
00:14:37.026 --> 00:14:39.706
that we've described,
and we're free

298
00:14:39.706 --> 00:14:42.626
to at this point
release the CGImage ref.

299
00:14:44.736 --> 00:14:46.666
The reverse is just as easy,

300
00:14:46.666 --> 00:14:49.866
going from a vImage
buffer to a CGImage ref.

301
00:14:50.876 --> 00:14:52.706
So we've done our
image processing,

302
00:14:53.606 --> 00:14:56.046
and we have our vImage
buffer out buffer.

303
00:14:56.796 --> 00:14:58.506
We haven't changed the
format so we're going

304
00:14:58.506 --> 00:15:01.676
to use our same format specifier
that we created before.

305
00:14:58.506 --> 00:15:01.676
to use our same format specifier
that we created before.

306
00:15:02.386 --> 00:15:05.536
To create the CGImage
refer we're going

307
00:15:05.536 --> 00:15:09.456
to call vImageCreate
CGImage ref from buffer.

308
00:15:09.926 --> 00:15:16.006
The first argument is going
to be the output vImage buffer

309
00:15:16.006 --> 00:15:17.666
that we just finished
processing,

310
00:15:18.976 --> 00:15:21.936
that same format type, because
we haven't changed the format.

311
00:15:23.156 --> 00:15:27.136
The next two arguments are user
callback and user functions,

312
00:15:27.966 --> 00:15:30.036
user callback functions
and user data.

313
00:15:30.926 --> 00:15:33.166
For this particular
conversion we don't need

314
00:15:33.166 --> 00:15:34.976
that so we're just
going to pass null.

315
00:15:35.496 --> 00:15:38.706
And then we pass flag,
any additional flags.

316
00:15:38.706 --> 00:15:40.616
Again, in this case
there are none,

317
00:15:40.676 --> 00:15:43.456
so we pass k at vImage,
no flags.

318
00:15:44.616 --> 00:15:47.056
And then finally a
reference to a vImage error

319
00:15:47.436 --> 00:15:50.616
to capture the error state.

320
00:15:50.796 --> 00:15:52.876
Upon successful return
of this function,

321
00:15:53.176 --> 00:15:56.066
we're going to return
the CGImage ref,

322
00:15:56.066 --> 00:15:57.646
out image in this case.

323
00:15:58.536 --> 00:16:03.336
This is going to be a freshly
allocated CGImage ref containing

324
00:15:58.536 --> 00:16:03.336
This is going to be a freshly
allocated CGImage ref containing

325
00:16:03.486 --> 00:16:07.246
the image information,
and we are free

326
00:16:07.246 --> 00:16:08.796
to release the vImagem buffer.

327
00:16:09.496 --> 00:16:15.136
All of this is built
around a really powerful API

328
00:16:15.136 --> 00:16:20.206
that we're introducing now
called vImage Convert Any

329
00:16:20.916 --> 00:16:22.926
to Any.

330
00:16:23.116 --> 00:16:25.456
What vImage Convert Any
to Any does it it converts

331
00:16:25.456 --> 00:16:28.756
between the image format
specifiers that we just saw,

332
00:16:29.246 --> 00:16:34.506
so you'll create two of these
format types, one for the source

333
00:16:34.506 --> 00:16:35.926
and one for the destination
type,

334
00:16:35.986 --> 00:16:37.836
and you'll create a converter.

335
00:16:39.036 --> 00:16:41.446
Once you've created this
converter, you can then convert

336
00:16:41.446 --> 00:16:44.266
as many images as you
want from that source type

337
00:16:44.266 --> 00:16:46.186
to that destination type.

338
00:16:46.736 --> 00:16:49.546
So this is one of those cases
where you want to create

339
00:16:49.546 --> 00:16:52.976
that converter once and use
it as many times as you can.

340
00:16:53.576 --> 00:16:58.556
The vImage Convert Any
to Any is really fast,

341
00:16:59.336 --> 00:17:01.216
and I want to show
you an example of hits

342
00:16:59.336 --> 00:17:01.216
and I want to show
you an example of hits

343
00:17:01.216 --> 00:17:02.576
with a real world application.

344
00:17:02.706 --> 00:17:08.715
I want to show you that with
software jpeg encode performance

345
00:17:09.106 --> 00:17:10.406
running on the iPhone 5.

346
00:17:10.986 --> 00:17:13.646
What I have here is a graph.

347
00:17:14.266 --> 00:17:16.726
On the y-axis I've got
megapixels per second,

348
00:17:16.826 --> 00:17:18.876
so this is the rate at
which we can perform

349
00:17:18.876 --> 00:17:20.296
that software jpeg encode.

350
00:17:21.296 --> 00:17:24.386
On the x-axis I have
various image format types.

351
00:17:24.596 --> 00:17:26.236
For the sake of this example,

352
00:17:26.236 --> 00:17:28.406
think of this software
jpeg encode

353
00:17:28.496 --> 00:17:30.036
as happening in two steps.

354
00:17:31.086 --> 00:17:33.866
Step one is to convert from
our input image format type,

355
00:17:33.996 --> 00:17:35.856
so those that we
see on the x-axis;

356
00:17:36.656 --> 00:17:40.976
two the image format type
that the encode step consumes,

357
00:17:41.786 --> 00:17:44.386
and the second step is to
perform the actual encode.

358
00:17:45.046 --> 00:17:49.086
What we're interested here
is step one, so converting

359
00:17:49.086 --> 00:17:50.606
from the input image format type

360
00:17:51.036 --> 00:17:53.426
to the format type
consumed by the encode.

361
00:17:54.476 --> 00:17:56.826
Let's take a look at the
performance the original way.

362
00:17:57.536 --> 00:18:01.786
We see a few things here.

363
00:17:57.536 --> 00:18:01.786
We see a few things here.

364
00:18:02.266 --> 00:18:03.976
First we see a lot
of variability.

365
00:18:05.146 --> 00:18:09.116
For example, if you start
from an 8-bit RGBA image,

366
00:18:09.606 --> 00:18:12.616
your encode performance is
going to be almost twice as fast

367
00:18:12.616 --> 00:18:15.516
as if you start from a
floating point RGBA image.

368
00:18:16.056 --> 00:18:19.156
The reason that this
is happening is

369
00:18:19.196 --> 00:18:21.276
because step one is so variable.

370
00:18:22.676 --> 00:18:25.346
So what we wanted to do
is change just step one.

371
00:18:25.536 --> 00:18:28.436
We replace step one now with
vImage Convert Any to Any,

372
00:18:29.856 --> 00:18:31.196
and let's look at
the performance.

373
00:18:35.356 --> 00:18:37.526
We see everything
gets a lot faster now.

374
00:18:38.926 --> 00:18:43.266
We also see that the
performance is quite consistent.

375
00:18:46.286 --> 00:18:50.306
So our 8-bit RGBA image is
now only a few percent faster

376
00:18:50.306 --> 00:18:52.306
than our floating
point RGBA image.

377
00:18:52.856 --> 00:18:56.746
The reason that this happens is
because we reduced the amount

378
00:18:56.746 --> 00:18:58.416
of time that we spent
in step one,

379
00:18:58.416 --> 00:19:01.116
converting from the input image
format to the other format,

380
00:18:58.416 --> 00:19:01.116
converting from the input image
format to the other format,

381
00:19:01.706 --> 00:19:04.266
to a very small percent
of the overall operation.

382
00:19:04.786 --> 00:19:09.836
This type of result is what you
can expect in your applications.

383
00:19:09.836 --> 00:19:11.536
This is a real world
application.

384
00:19:12.596 --> 00:19:14.736
vImage is delivering
great performance

385
00:19:14.866 --> 00:19:16.236
and consistent results.

386
00:19:16.236 --> 00:19:20.876
I want to stay on the
topic of conversion

387
00:19:20.876 --> 00:19:21.886
for a little bit longer.

388
00:19:22.676 --> 00:19:25.336
I want to talk about an example

389
00:19:25.336 --> 00:19:27.936
of scaling a premultiplied
image.

390
00:19:28.506 --> 00:19:33.456
A lot of people will have an
image format and they'll have it

391
00:19:33.456 --> 00:19:36.136
in a vImage buffer and
they'll want to scale it.

392
00:19:36.526 --> 00:19:37.816
They'll look through
vImagem and see

393
00:19:37.816 --> 00:19:43.126
that the only way you can scale
an image is a non-premultiplied

394
00:19:43.126 --> 00:19:43.826
image format.

395
00:19:44.926 --> 00:19:48.366
So the way that you need to do
this is three steps in vImage.

396
00:19:49.456 --> 00:19:51.806
I'm not going to go into the
details of each of these steps,

397
00:19:52.146 --> 00:19:59.386
but in step one, we're going
to unpremultiply the data.

398
00:19:59.566 --> 00:20:02.656
In step two, we're going
to perform the scale.

399
00:19:59.566 --> 00:20:02.656
In step two, we're going
to perform the scale.

400
00:20:03.926 --> 00:20:05.706
And then in step
three we're going

401
00:20:05.706 --> 00:20:08.176
to premultiply the
results of that output.

402
00:20:08.726 --> 00:20:12.906
A lot of people see this as
three times the amount of work,

403
00:20:14.006 --> 00:20:15.906
and they get afraid
and they go off

404
00:20:15.906 --> 00:20:17.336
and they implement
their own scale.

405
00:20:17.336 --> 00:20:21.496
I want to show you how much time
we spent in each of these steps.

406
00:20:22.026 --> 00:20:26.456
What I have here is the
percentage of time in each

407
00:20:26.456 --> 00:20:28.396
of those three same
steps as we saw them.

408
00:20:28.976 --> 00:20:32.746
At the top we see
unpremultiply, a little over 1%,

409
00:20:32.746 --> 00:20:37.756
at the bottom we see the
premultiply, a little of 1/2%.

410
00:20:39.116 --> 00:20:42.016
The vast majority of time is
spent in the actual operation.

411
00:20:42.566 --> 00:20:44.596
What I want you to take away

412
00:20:44.596 --> 00:20:47.816
from this is don't take away
the conversions, they're fast.

413
00:20:48.436 --> 00:20:51.446
If your image isn't in the right
format, use the conversions.

414
00:20:51.446 --> 00:20:53.996
It's going to be worthwhile
getting into the image.

415
00:20:53.996 --> 00:21:00.986
Now I want to talk about
some performance of vImage

416
00:20:53.996 --> 00:21:00.986
Now I want to talk about
some performance of vImage

417
00:21:00.986 --> 00:21:04.736
as compared to some of the
other options, and I want to do

418
00:21:04.736 --> 00:21:07.156
that by comparing to OpenCV.

419
00:21:08.076 --> 00:21:11.946
OpenCV is a third party open
source computer vision library.

420
00:21:13.216 --> 00:21:14.996
It has an image processing
module.

421
00:21:15.766 --> 00:21:17.546
That image processing
module has a lot

422
00:21:17.546 --> 00:21:19.516
of the same functionality
that vImage has.

423
00:21:19.996 --> 00:21:23.426
There's a couple points
that I want to compare.

424
00:21:24.076 --> 00:21:25.826
The first is execution time.

425
00:21:27.196 --> 00:21:29.436
Everybody wants their
applications to run fast.

426
00:21:30.356 --> 00:21:31.906
The second is energy consumed.

427
00:21:32.616 --> 00:21:35.546
We're increasingly reliant on
our batteries so it's important

428
00:21:35.546 --> 00:21:37.976
that we get that
performance while being aware

429
00:21:37.976 --> 00:21:39.226
of the energy consumption.

430
00:21:39.796 --> 00:21:43.856
To begin we'll look at the
execution time and we'll do

431
00:21:43.856 --> 00:21:47.456
that by looking at the
speedup of vImage over OpenCV.

432
00:21:48.726 --> 00:21:52.646
So on this graph I've
got numbers where numbers

433
00:21:52.646 --> 00:21:56.296
above 1 means vImage is going
to be that many times faster

434
00:21:56.296 --> 00:22:00.256
than OpenCV, and for numbers
below 1 it means OpenCV is going

435
00:21:56.296 --> 00:22:00.256
than OpenCV, and for numbers
below 1 it means OpenCV is going

436
00:22:00.256 --> 00:22:00.846
to be faster.

437
00:22:01.986 --> 00:22:05.236
I've got a handful of operations
here, and we see that vImage is

438
00:22:05.236 --> 00:22:10.006
between 1.6 and over 20
times faster than OpenCV,

439
00:22:10.956 --> 00:22:13.506
so these are some really
great performance results.

440
00:22:13.926 --> 00:22:16.386
But as I mentioned, it's not
just all about performance.

441
00:22:17.676 --> 00:22:20.736
We're concerned also with energy
consumption and battery life.

442
00:22:20.736 --> 00:22:25.706
I want to explain this
relationship between performance

443
00:22:25.706 --> 00:22:29.606
and energy consumption and
battery life a little bit,

444
00:22:29.606 --> 00:22:30.666
and there's a few points.

445
00:22:30.936 --> 00:22:34.666
First, fast code tends to
decrease energy consumption,

446
00:22:35.736 --> 00:22:38.466
therefore, fast code tends
to increase battery life.

447
00:22:39.616 --> 00:22:41.626
Let's look at why
this tends to happen.

448
00:22:42.896 --> 00:22:46.216
What I have here is a typical
energy consumption profile.

449
00:22:46.636 --> 00:22:48.896
So we're measuring the
instantaneous power.

450
00:22:50.336 --> 00:22:52.636
Energy is the area
underneath that power curve.

451
00:22:53.206 --> 00:22:56.156
So on the x-axis I've got time.

452
00:22:57.616 --> 00:23:01.186
In the beginning, on the y-axis
I've got our instantaneous

453
00:22:57.616 --> 00:23:01.186
In the beginning, on the y-axis
I've got our instantaneous

454
00:23:01.186 --> 00:23:01.896
power measurement.

455
00:23:03.156 --> 00:23:05.096
In the beginning we're
running at some idle state

456
00:23:05.096 --> 00:23:08.116
and using a very
small amount of power.

457
00:23:08.116 --> 00:23:10.276
At time t0 our application
begins

458
00:23:10.686 --> 00:23:13.676
and we increase the amount of
power that we're consuming.

459
00:23:14.246 --> 00:23:16.106
The application runs
through time t1

460
00:23:16.106 --> 00:23:18.796
and we return back
to some idle state.

461
00:23:19.456 --> 00:23:23.366
The amount of battery that we're
using, the energy consumption,

462
00:23:23.366 --> 00:23:25.076
is the area underneath
this curve.

463
00:23:25.786 --> 00:23:27.986
Let's look at how an
optimized routine compares

464
00:23:27.986 --> 00:23:29.336
to an unoptimized routine.

465
00:23:29.926 --> 00:23:34.256
So here in blue I've got
an optimized routine --

466
00:23:34.846 --> 00:23:35.806
much faster.

467
00:23:36.476 --> 00:23:41.156
In certain situations it's
going to take more power to make

468
00:23:41.156 --> 00:23:45.216
that routine run faster, but
the important part here is

469
00:23:45.436 --> 00:23:47.936
that the energy consumption
is the area underneath,

470
00:23:48.056 --> 00:23:50.626
and we can seek that the
optimized routine is using

471
00:23:50.626 --> 00:23:52.126
significantly less energy.

472
00:23:52.766 --> 00:23:58.176
So now let's look at that
same vImage OpenCV comparison

473
00:23:58.876 --> 00:24:00.416
for the energy numbers.

474
00:23:58.876 --> 00:24:00.416
for the energy numbers.

475
00:24:01.856 --> 00:24:05.216
So I've got the vImage energy
savings over OpenCV here.

476
00:24:06.596 --> 00:24:10.656
So again, numbers above
1 means vImage is using

477
00:24:10.656 --> 00:24:12.686
that much times less
energy than OpenCV,

478
00:24:12.686 --> 00:24:17.156
and for numbers below 1 it means
OpenCV is using less energy.

479
00:24:18.916 --> 00:24:24.036
This ranges from .75 up through
almost 7 times less energy.

480
00:24:25.246 --> 00:24:27.226
So we're delivering
really great performance,

481
00:24:27.226 --> 00:24:30.496
and we're also delivering
really great energy savings.

482
00:24:31.456 --> 00:24:33.716
This is what you can
expect in your applications.

483
00:24:33.716 --> 00:24:40.436
We love to get feedback about
use of the Accelerate Framework

484
00:24:40.436 --> 00:24:44.066
and we found this tweet I wanted
to share with you: "Using vImage

485
00:24:44.096 --> 00:24:45.556
from the Accelerate Framework

486
00:24:45.556 --> 00:24:47.496
to dynamically prerender
my spreads,

487
00:24:48.776 --> 00:24:54.216
it's the only way
to make it fast."

488
00:24:54.976 --> 00:24:57.326
Now I want to move on
to the next big category

489
00:24:57.426 --> 00:24:59.596
of operations available on
the Accelerate Framework

490
00:24:59.596 --> 00:25:02.346
and that is digital
signal processing.

491
00:24:59.596 --> 00:25:02.346
and that is digital
signal processing.

492
00:25:02.906 --> 00:25:06.606
You'll find digital
signal processing in vDSP.

493
00:25:06.646 --> 00:25:10.136
This is our Vectorized Digital
Signal Processing library.

494
00:25:10.756 --> 00:25:17.166
In vDSP you'll find basic
operation on arrays, additions,

495
00:25:17.166 --> 00:25:22.206
subtractions, multiplies,
conversions, accumulations.

496
00:25:23.496 --> 00:25:26.326
You'll also find discrete
Fourier transforms,

497
00:25:26.326 --> 00:25:28.086
discrete cosine transforms,

498
00:25:28.556 --> 00:25:31.036
as well as convolutions
and correlations.

499
00:25:31.596 --> 00:25:36.656
In both iOS 7 and OS 10.9,

500
00:25:36.656 --> 00:25:39.056
we've introduced some great
new features and functionality.

501
00:25:39.716 --> 00:25:43.936
The first of these is a
multi-channel IIR filter.

502
00:25:44.076 --> 00:25:46.406
This is an infinite
impulse response filter.

503
00:25:46.976 --> 00:25:51.016
So whereas before if you
needed to perform an IIR filter

504
00:25:51.016 --> 00:25:54.336
on multiple channels, maybe you
have a surround sound system

505
00:25:54.336 --> 00:25:56.246
that you want to
filter, you'd have to do

506
00:25:56.246 --> 00:26:00.296
that with individual
calls into an IIR filter.

507
00:25:56.246 --> 00:26:00.296
that with individual
calls into an IIR filter.

508
00:26:00.456 --> 00:26:02.136
Now with this new
multi-channel you can do

509
00:26:02.136 --> 00:26:05.426
that with a single function
call, and we've been able

510
00:26:05.426 --> 00:26:08.656
to give you some great
performance and energy savings

511
00:26:08.686 --> 00:26:11.256
by doing that operation
in a single function.

512
00:26:15.216 --> 00:26:17.286
We've also improved
power of 2 support

513
00:26:17.286 --> 00:26:19.636
for the discrete
Fourier transform

514
00:26:19.636 --> 00:26:21.556
and the discrete
cosine transform.

515
00:26:21.896 --> 00:26:24.706
I want to talk about
this with an example.

516
00:26:25.326 --> 00:26:32.026
So before we essentially
had two entry points

517
00:26:32.376 --> 00:26:35.146
for the same operation based
on the number of points

518
00:26:35.146 --> 00:26:36.316
that you wanted to evaluate.

519
00:26:38.116 --> 00:26:40.696
So if you had a power of 2,
you would call into the FFT.

520
00:26:40.696 --> 00:26:45.856
If you had a non-power of 2
you would call into the DFT.

521
00:26:45.976 --> 00:26:48.876
Starting in iOS 10.9 and iOS 7,

522
00:26:48.876 --> 00:26:53.456
the DFT supports
certain powers of 2.

523
00:26:53.696 --> 00:26:56.006
When the DFT supports the
number of points that you want

524
00:26:56.006 --> 00:27:00.666
to compute, we recommend
that you use the DFT.

525
00:26:56.006 --> 00:27:00.666
to compute, we recommend
that you use the DFT.

526
00:27:00.876 --> 00:27:03.306
So this brings up another
question: How can I be sure

527
00:27:03.306 --> 00:27:04.896
that my number of
points is supported?

528
00:27:05.736 --> 00:27:08.226
If you can't find it in the
documentation for some reason,

529
00:27:08.566 --> 00:27:10.156
you can always programmatically
check.

530
00:27:11.146 --> 00:27:14.506
The DFT is one of the routines
that requires a setup structure,

531
00:27:15.146 --> 00:27:18.136
and that setup structure
is designed to return 0

532
00:27:18.136 --> 00:27:19.816
if the number of
points isn't supported.

533
00:27:20.276 --> 00:27:21.536
You can always be sure

534
00:27:21.766 --> 00:27:24.276
that you're using
the correct routine.

535
00:27:24.276 --> 00:27:29.666
Let's look at an
example of the DFT.

536
00:27:31.156 --> 00:27:33.436
Again, we'll start by including
the Accelerate Framework,

537
00:27:34.756 --> 00:27:36.776
then we're going to create
and prepare our data.

538
00:27:36.776 --> 00:27:40.446
In this case we've got 4
buffers, 2 input buffers,

539
00:27:41.126 --> 00:27:46.406
one for the real numbers and
one for the imaginary numbers,

540
00:27:46.696 --> 00:27:48.836
2 output buffers --
again, one for the real

541
00:27:48.836 --> 00:27:49.916
and one for the imaginary.

542
00:27:50.916 --> 00:27:52.576
We want to align
these if possible.

543
00:27:53.126 --> 00:27:58.526
Then we're going to perform a
DFT setup, and we're going to do

544
00:27:58.526 --> 00:28:00.616
that with vDSP zop create setup.

545
00:27:58.526 --> 00:28:00.616
that with vDSP zop create setup.

546
00:28:01.896 --> 00:28:03.076
Takes a few arguments.

547
00:28:03.796 --> 00:28:05.926
The first argument
is information

548
00:28:05.926 --> 00:28:08.076
about any pervious setups
that may have occurred.

549
00:28:08.256 --> 00:28:11.736
We don't have one in this case
so we'll pass zero or null.

550
00:28:12.536 --> 00:28:15.596
The next is the number of points
that we want to compute, 1024,

551
00:28:15.596 --> 00:28:19.976
and then information that
describes the DFT that we want

552
00:28:20.306 --> 00:28:23.566
to perform, in this
case the forward DFT.

553
00:28:24.726 --> 00:28:29.326
Once we've created a setup,
we're going to execute our DFT.

554
00:28:30.396 --> 00:28:33.836
We do that with vDSP
DFT execute,

555
00:28:33.836 --> 00:28:35.696
takes that setup structure
that we just created

556
00:28:35.696 --> 00:28:39.996
and the 4 buffers that
we had set up before.

557
00:28:39.996 --> 00:28:44.066
Again, we want to do this
as many times as we can

558
00:28:44.066 --> 00:28:45.826
with that same setup structure.

559
00:28:45.906 --> 00:28:47.436
We can use it over
and over again.

560
00:28:48.546 --> 00:28:51.276
Once we've done all the
computation one time at the end,

561
00:28:51.276 --> 00:28:55.306
then we want to clean up our
setup with vDSP DFT Destroy.

562
00:28:56.046 --> 00:29:02.596
So I want to do another
comparison now vDSP versus FFTW.

563
00:28:56.046 --> 00:29:02.596
So I want to do another
comparison now vDSP versus FFTW.

564
00:29:04.126 --> 00:29:07.886
FFTW is called Fastest
Fourier Transform in the West.

565
00:29:09.016 --> 00:29:13.086
This is another third party
freely available library,

566
00:29:13.826 --> 00:29:14.536
supports one

567
00:29:14.536 --> 00:29:16.446
and multidimensional
transformations,

568
00:29:17.356 --> 00:29:19.116
both real and complex data.

569
00:29:19.836 --> 00:29:20.606
It's parallel.

570
00:29:21.176 --> 00:29:24.936
It's a good freely
available library.

571
00:29:24.936 --> 00:29:26.126
It's a fair comparison.

572
00:29:30.286 --> 00:29:33.126
I'm going to show
again the vDSP speedup

573
00:29:33.126 --> 00:29:35.746
over FFTW on the iPhone 5.

574
00:29:37.206 --> 00:29:40.396
So again, numbers above 1
means vDSP is going to be

575
00:29:40.396 --> 00:29:43.366
that many times faster than FFTW

576
00:29:43.976 --> 00:29:46.056
and numbers below
1 FFTW is going

577
00:29:46.056 --> 00:29:47.816
to be faster than the vDSP.

578
00:29:48.616 --> 00:29:54.766
Across the x-axis I have
several number of points

579
00:29:54.766 --> 00:29:55.866
that we're going to execute.

580
00:29:56.676 --> 00:29:58.536
Let's take a look at the
performance that we get.

581
00:29:58.536 --> 00:30:06.836
We see that vDSP is between
1.8 and about 2.5 times faster

582
00:29:58.536 --> 00:30:06.836
We see that vDSP is between
1.8 and about 2.5 times faster

583
00:30:06.886 --> 00:30:09.606
than FFTW for all of these
number of points that we looked

584
00:30:09.606 --> 00:30:12.336
at -- some really great
performance results.

585
00:30:12.876 --> 00:30:16.376
It's one thing to look
at benchmarks, though.

586
00:30:17.476 --> 00:30:19.986
It's another thing to
look at the performance

587
00:30:19.986 --> 00:30:22.586
that you can expect
from a real application.

588
00:30:23.166 --> 00:30:28.536
So imagine you need to code an
audio signal using AAC enhanced

589
00:30:28.656 --> 00:30:29.156
low delay.

590
00:30:30.656 --> 00:30:34.066
This is a process
that's done in face time.

591
00:30:34.846 --> 00:30:38.336
The DFT is one of many of
the DFT routines in use,

592
00:30:38.386 --> 00:30:40.026
but it's the only one that
we're looking at here.

593
00:30:40.026 --> 00:30:43.346
And we're going to look at this
by looking at the percentage

594
00:30:43.346 --> 00:30:49.696
of time that we spend
in the DFT.

595
00:30:49.906 --> 00:30:52.636
So what I've got here is the
percentage of time for the DFT

596
00:30:52.636 --> 00:30:58.706
at 54% and at 47% is everything
else in the operation.

597
00:30:59.196 --> 00:31:02.816
This is when we're
linking against FFTW.

598
00:30:59.196 --> 00:31:02.816
This is when we're
linking against FFTW.

599
00:31:02.816 --> 00:31:07.156
The only thing we change
is we link against vDSP

600
00:31:07.156 --> 00:31:10.236
so that we get the
DFT out of vDSP.

601
00:31:10.236 --> 00:31:13.086
And let's look at
how this changes.

602
00:31:17.116 --> 00:31:20.036
When the DFT is replaced
with the DFT out of VDSP,

603
00:31:20.036 --> 00:31:22.676
the time spent goes to 30%.

604
00:31:23.446 --> 00:31:26.576
This translates to significant
performance and energy savings.

605
00:31:27.966 --> 00:31:30.346
This is what you can
expect in your applications.

606
00:31:30.886 --> 00:31:37.766
A little bit more details
about what VDSP supports.

607
00:31:38.296 --> 00:31:42.386
It supports single and
double precision, both real

608
00:31:42.386 --> 00:31:45.956
and complex values,
as well as strided

609
00:31:45.956 --> 00:31:47.596
and non-strided data accesses.

610
00:31:48.166 --> 00:31:52.386
So again, we love
to get feedback.

611
00:31:52.386 --> 00:31:55.276
Another tweet about using vDSP.

612
00:31:55.566 --> 00:31:57.166
Want to do FFT on iOS?

613
00:31:57.416 --> 00:31:58.776
Use the Accelerate Framework.

614
00:31:59.226 --> 00:32:00.296
Highly recommended.

615
00:31:59.226 --> 00:32:00.296
Highly recommended.

616
00:32:01.516 --> 00:32:02.006
Thank you.

617
00:32:02.586 --> 00:32:06.686
So now I want to move on to
transcendental math functions.

618
00:32:06.926 --> 00:32:08.706
And for that, I'm going
to turn it over to Luke.

619
00:32:10.146 --> 00:32:10.786
>> Luke: Hello, everyone.

620
00:32:10.786 --> 00:32:12.376
My name's Luke Chang.

621
00:32:12.896 --> 00:32:14.826
I'm here to talk
about math functions.

622
00:32:15.676 --> 00:32:19.936
In our group, we support
math for every data level.

623
00:32:20.966 --> 00:32:26.116
For scaled data, we have
libem, takes a scalar input,

624
00:32:26.226 --> 00:32:28.286
returns a scalar output.

625
00:32:29.256 --> 00:32:33.566
If you're writing vector
code, we have the method.

626
00:32:34.786 --> 00:32:36.656
It takes a SIMD vector S input

627
00:32:36.656 --> 00:32:39.236
and then return a
SIMD vector S output.

628
00:32:39.736 --> 00:32:44.656
And you want to handle a lot
of data, will have vForce.

629
00:32:45.786 --> 00:32:48.766
It takes Arias input and
then returns Arias output.

630
00:32:48.766 --> 00:32:51.956
We're going to talk
about them one by one.

631
00:32:52.756 --> 00:32:54.556
First, libem.

632
00:32:54.936 --> 00:33:00.766
It's a standard C math
library, it has a collection

633
00:32:54.936 --> 00:33:00.766
It's a standard C math
library, it has a collection

634
00:33:00.766 --> 00:33:05.426
of [inaudible] like
exponents, logarithm,

635
00:33:06.286 --> 00:33:08.056
trigonometry, power functions.

636
00:33:09.036 --> 00:33:12.206
You're probably very familiar
with it, so I'm going to talk

637
00:33:12.206 --> 00:33:14.806
about what we added
this year for libem.

638
00:33:15.366 --> 00:33:20.476
What we added is an
extension to the C11 standard,

639
00:33:20.476 --> 00:33:23.766
so we prefixed the function
name with double underscores.

640
00:33:25.046 --> 00:33:30.056
They are available on both
iOS 7 and Mac OS 10.9.

641
00:33:30.856 --> 00:33:35.696
They are power of 10 function,
trigonometry in terms of pi,

642
00:33:36.746 --> 00:33:38.206
and sine and cosine pairs.

643
00:33:38.676 --> 00:33:44.386
First, power of 10, why
do we add power of 10?

644
00:33:45.486 --> 00:33:48.226
It's a very common operation
in decimal calculation,

645
00:33:48.976 --> 00:33:52.586
so if you're writing audio apps,
you need quite a lot of it.

646
00:33:53.586 --> 00:33:57.946
Without a specific power of 10
function you have 2 options --

647
00:33:59.026 --> 00:34:03.166
one, to use Pow and use
constant 10 as base.

648
00:33:59.026 --> 00:34:03.166
one, to use Pow and use
constant 10 as base.

649
00:34:04.046 --> 00:34:08.295
However, this is inefficient,
because Pow is designed

650
00:34:08.295 --> 00:34:09.716
to handle generic inputs.

651
00:34:10.766 --> 00:34:13.786
if you know your base is a
constant, there are a lot

652
00:34:13.786 --> 00:34:18.116
of optimization that we can
do to make it go faster.

653
00:34:18.896 --> 00:34:21.235
The other way is to use X.

654
00:34:22.326 --> 00:34:27.136
You can prescale your input
by log(10) to do power of 10.

655
00:34:28.456 --> 00:34:30.746
But it has its own problem.

656
00:34:31.146 --> 00:34:31.976
It's not accurate.

657
00:34:32.436 --> 00:34:34.876
There's routing error
in the multiplication.

658
00:34:35.766 --> 00:34:39.815
For example, if you
want to calculate 10(5),

659
00:34:40.406 --> 00:34:44.726
using this method, you will
not exactly get 100,000.

660
00:34:45.356 --> 00:34:46.966
There's a small error
at the end.

661
00:34:48.456 --> 00:34:52.406
That's why we added
X(10) so you can do power

662
00:34:52.406 --> 00:34:56.085
of 10 faster and more accurate.

663
00:34:57.266 --> 00:35:00.486
Next is trigonometry
function in terms of pi.

664
00:34:57.266 --> 00:35:00.486
Next is trigonometry
function in terms of pi.

665
00:35:01.836 --> 00:35:04.926
Basically it's the same
regular trigonometry function

666
00:35:04.926 --> 00:35:06.856
with your input scale by pi.

667
00:35:07.546 --> 00:35:12.076
It is faster because we can do
automatic reductions faster.

668
00:35:12.566 --> 00:35:16.246
It's much easier to reduce
the argument by multiple of 2

669
00:35:16.786 --> 00:35:20.576
than multiple of 2 pi.

670
00:35:20.576 --> 00:35:23.946
It's also more accurate when
you're dealing with degrees.

671
00:35:25.256 --> 00:35:29.576
For example, if you want to
calculate cosine of 90 degrees,

672
00:35:30.716 --> 00:35:32.726
90 degrees [inaudible]
into 1/2 pi.

673
00:35:33.696 --> 00:35:36.076
With the regular trigonometry
function you will have

674
00:35:36.076 --> 00:35:42.576
to say cos pi x 0.5, and
you will not get 0 back;

675
00:35:42.576 --> 00:35:44.056
you will get a very
small number,

676
00:35:44.646 --> 00:35:48.026
because pi is not so accurate.

677
00:35:49.386 --> 00:35:56.016
So if you use cos pi 0.5,
you will get exactly 0 back.

678
00:35:56.556 --> 00:36:01.936
There's no error
sine/cosine pairs.

679
00:35:56.556 --> 00:36:01.936
There's no error
sine/cosine pairs.

680
00:36:03.276 --> 00:36:05.096
A lot of times when
you can't really sine,

681
00:36:05.096 --> 00:36:07.656
you'll need cosine
for the same value.

682
00:36:08.086 --> 00:36:11.826
For example, if you want to do
a polar 2 [inaudible] conversion

683
00:36:11.826 --> 00:36:17.726
you will need cosine for the
x-axis and sine for the y-axis.

684
00:36:19.956 --> 00:36:22.276
Because we do it simultaneously,

685
00:36:22.506 --> 00:36:24.296
there is only one
argument reduction.

686
00:36:24.856 --> 00:36:28.196
You will have to do the argument
reduction twice to save time.

687
00:36:28.196 --> 00:36:31.366
And what's even better is

688
00:36:31.366 --> 00:36:34.026
that compiler recognize
we have sine cos,

689
00:36:34.546 --> 00:36:39.316
so you will optimize your
code into calling sine cos,

690
00:36:39.776 --> 00:36:40.866
without even knowing it.

691
00:36:41.716 --> 00:36:46.646
Of course, if you want to call
sine cos yourself, you can.

692
00:36:47.356 --> 00:36:52.966
We also added C11
support for CMPLX.

693
00:36:54.846 --> 00:36:57.576
This macro is used to
define a complex number.

694
00:36:58.916 --> 00:37:01.176
Without this, you're more likely

695
00:36:58.916 --> 00:37:01.176
Without this, you're more likely

696
00:37:01.176 --> 00:37:05.716
to do the real part
+ imaginary part x I.

697
00:37:06.696 --> 00:37:10.836
But in that expression, there's
addition and a multiplication

698
00:37:10.836 --> 00:37:15.376
in it, so sometimes you will
not get what you expect --

699
00:37:15.376 --> 00:37:19.116
like this example:
0.0 + infinity x I.

700
00:37:20.876 --> 00:37:24.866
Using CMPLX allows you
to specify the real part

701
00:37:24.866 --> 00:37:27.976
and the imaginary part of
the complex number directly,

702
00:37:28.516 --> 00:37:30.466
so you don't have to worry
about multiplication.

703
00:37:31.016 --> 00:37:37.786
We also have CMPLXF and CMPLXL
for float and load level.

704
00:37:39.196 --> 00:37:41.766
So that's the new
addition to libem.

705
00:37:42.896 --> 00:37:46.566
Vmathlib is a SIMD
vector math library.

706
00:37:47.416 --> 00:37:51.406
It is designed to take
a SIMD vector as input

707
00:37:51.486 --> 00:37:53.236
and then return a SIMD vector.

708
00:37:54.926 --> 00:37:57.196
Similar to libem,
it has a collection

709
00:37:57.196 --> 00:37:57.796
of [inaudible] functions.

710
00:37:58.516 --> 00:38:04.436
We prefix the function then
with a single V, so we have VX,

711
00:37:58.516 --> 00:38:04.436
We prefix the function then
with a single V, so we have VX,

712
00:38:04.436 --> 00:38:06.986
Vlog, Vsine, et cetera.

713
00:38:07.426 --> 00:38:11.146
You want to use V method

714
00:38:11.146 --> 00:38:12.846
when you're writing
your own vector code.

715
00:38:14.136 --> 00:38:18.626
Accelerate Framework provide a
wide range of functionalities,

716
00:38:19.236 --> 00:38:21.766
but sometimes you have
your own special algorithm

717
00:38:21.766 --> 00:38:24.176
that you write, and
you want to be fast,

718
00:38:24.606 --> 00:38:25.746
so you write in vector code.

719
00:38:26.176 --> 00:38:28.926
What if you need
the, for example?

720
00:38:31.126 --> 00:38:35.436
You could use libem and then
use a for loop to iterate

721
00:38:35.436 --> 00:38:39.116
through each of your
element in the SIMD vector.

722
00:38:39.896 --> 00:38:43.566
But obviously you're not
going to take full advantage

723
00:38:43.566 --> 00:38:49.346
of the vector unit, so we
can replace it with Vmathlib.

724
00:38:51.796 --> 00:38:55.846
Instead of including Math.H,
you include accelerator header,

725
00:38:55.846 --> 00:38:58.486
accelerator.h. Instead of the

726
00:38:58.546 --> 00:39:02.326
for loop you make one
function call to VsineF.

727
00:38:58.546 --> 00:39:02.326
for loop you make one
function call to VsineF.

728
00:39:02.716 --> 00:39:04.156
You will take your SIMD vector

729
00:39:04.156 --> 00:39:07.256
and then return the
result SIMD vector.

730
00:39:07.846 --> 00:39:09.566
But you can go on
with your vector code.

731
00:39:10.996 --> 00:39:14.506
The code looks simpler,
cleaner, and it's also faster.

732
00:39:14.506 --> 00:39:16.656
So it's VMathlib.

733
00:39:16.656 --> 00:39:20.986
You use it when you write
your own vector code.

734
00:39:21.896 --> 00:39:23.656
Next, vForce.

735
00:39:24.676 --> 00:39:27.116
vForce is designed to
handle a lot of data,

736
00:39:27.816 --> 00:39:29.896
called the vectorized
math library.

737
00:39:30.506 --> 00:39:34.076
It works on arrays, so it
prefix the function then

738
00:39:34.076 --> 00:39:39.246
with double Vs, VVX,
VVlog, VVSine, et cetera.

739
00:39:39.246 --> 00:39:44.486
Let's say you want to write
a signal generator app

740
00:39:44.486 --> 00:39:47.166
and you want to generate
a sine wave, for example.

741
00:39:47.736 --> 00:39:52.366
You can do it with Libem,
again, write a for loop,

742
00:39:52.366 --> 00:39:54.336
go through each element
in your buffer --

743
00:39:54.906 --> 00:39:58.636
you could do better
by using vForce.

744
00:39:59.536 --> 00:40:00.036
Here's how.

745
00:39:59.536 --> 00:40:00.036
Here's how.

746
00:40:02.296 --> 00:40:04.036
Instead of using a for loop,

747
00:40:04.706 --> 00:40:07.056
you make one function
call to VV Sine F.

748
00:40:08.136 --> 00:40:10.996
You're passing the upper
buffer, inner buffer,

749
00:40:10.996 --> 00:40:12.956
and the pointer to the length.

750
00:40:14.476 --> 00:40:17.466
The generator sine will be
ready in the upper buffer right

751
00:40:17.466 --> 00:40:18.636
after this function call.

752
00:40:19.086 --> 00:40:22.666
Again, the code looks
simpler, cleaner,

753
00:40:22.666 --> 00:40:26.196
and most importantly, is faster.

754
00:40:27.376 --> 00:40:30.616
Let's look at the performance
measured on the iPhone 5.

755
00:40:30.796 --> 00:40:36.406
As you can see, vForce
is more than twice faster

756
00:40:36.626 --> 00:40:37.666
than using a for loop.

757
00:40:38.546 --> 00:40:41.016
Within the same amount of
time it can generate more

758
00:40:41.016 --> 00:40:44.126
than twice the restful
than the for loop.

759
00:40:45.176 --> 00:40:46.196
This is not it.

760
00:40:47.056 --> 00:40:49.726
It also has great
energy performance.

761
00:40:50.616 --> 00:40:53.116
It use lot less energy
than using a for loop.

762
00:40:53.696 --> 00:40:58.736
It use about only
60% of the energy

763
00:41:00.016 --> 00:41:04.116
when you use vForce
compared to a for loop.

764
00:41:05.576 --> 00:41:10.396
So your app will last longer,
you will not drain the battery,

765
00:41:10.506 --> 00:41:15.116
and we did not cherry
pick just VVSineF

766
00:41:15.116 --> 00:41:16.246
to show you the performance.

767
00:41:16.686 --> 00:41:18.926
There is performance
improvement across the board.

768
00:41:19.726 --> 00:41:21.756
The graph doesn't even
fit into the screen.

769
00:41:22.146 --> 00:41:25.876
For the Trunk F, vForce is
more than 5 times faster

770
00:41:25.876 --> 00:41:27.106
than using a for loop.

771
00:41:27.546 --> 00:41:30.846
For all other functions they
are at least twice faster

772
00:41:30.846 --> 00:41:32.446
than using a for loop.

773
00:41:34.856 --> 00:41:36.916
A few words about vForce.

774
00:41:36.916 --> 00:41:38.646
vForce supports single

775
00:41:38.646 --> 00:41:40.436
and double precision
floating point numbers.

776
00:41:40.436 --> 00:41:44.926
It handle Edge cases currently,
so if you have infinities

777
00:41:44.926 --> 00:41:47.856
or nins in your input, you
don't have to worry about them.

778
00:41:48.656 --> 00:41:51.156
vForce will handle the
Edge cases correctly.

779
00:41:51.606 --> 00:41:55.476
vForce require minimal
data alignment.

780
00:41:56.226 --> 00:41:58.156
We only require native
data alignment

781
00:41:58.226 --> 00:42:01.646
for a single precision floating
number that's 4 bytes aligned,

782
00:41:58.226 --> 00:42:01.646
for a single precision floating
number that's 4 bytes aligned,

783
00:42:02.106 --> 00:42:04.656
double precision floating point
number is 8 bytes aligned.

784
00:42:05.676 --> 00:42:08.506
Supports in place
operation, so you don't have

785
00:42:08.506 --> 00:42:09.986
to create a temporary buffer.

786
00:42:09.986 --> 00:42:11.796
That minimize the
memory movement.

787
00:42:12.216 --> 00:42:16.146
We get this question a lot.

788
00:42:16.406 --> 00:42:20.876
Like Jeff mentioned before,
how much data is enough,

789
00:42:21.206 --> 00:42:25.826
so using vForce or any other
server function is beneficial?

790
00:42:27.686 --> 00:42:32.056
Well, for vForce, I can give
a rule of thumb; that is,

791
00:42:32.056 --> 00:42:36.306
if you have more than 16
elements in your array,

792
00:42:36.306 --> 00:42:37.486
consider using vForce.

793
00:42:38.126 --> 00:42:42.446
Of course, the actual crossover
point may vary for each function

794
00:42:42.446 --> 00:42:46.046
in vForce, but if you
have more than 16,

795
00:42:46.046 --> 00:42:49.006
you're probably good to go.

796
00:42:49.006 --> 00:42:49.996
So that's vForce.

797
00:42:49.996 --> 00:42:52.856
I'm going to hand the
presentation back to Jeff.

798
00:42:52.856 --> 00:42:54.316
He'll talk about linear algebra,

799
00:42:54.316 --> 00:42:56.186
my favorite section
of the presentation.

800
00:42:57.516 --> 00:43:00.816
[Applause]

801
00:42:57.516 --> 00:43:00.816
[Applause]

802
00:43:01.316 --> 00:43:01.826
>> Jeff: Thanks, Luke.

803
00:43:03.216 --> 00:43:06.546
So for linear algebra we've got
the industry standard LAPACK

804
00:43:06.716 --> 00:43:08.126
and BLAS libraries.

805
00:43:08.766 --> 00:43:10.846
LAPACK is linear
algebra package,

806
00:43:11.636 --> 00:43:14.356
and BLAS is basic linear
algebra subprograms.

807
00:43:15.266 --> 00:43:17.046
Let's begin with LAPACK.

808
00:43:17.876 --> 00:43:21.066
In LAPACK you'll find high level
linear algebra functionality.

809
00:43:22.206 --> 00:43:24.366
This includes things
like solving systems

810
00:43:24.366 --> 00:43:28.086
of linear equations, performing
matrix factorizations,

811
00:43:29.226 --> 00:43:31.946
as well as computing eigen
values and eigen vectors.

812
00:43:32.426 --> 00:43:37.346
One of the great ways to tell
how you're doing with LAPACK

813
00:43:37.346 --> 00:43:39.466
and BLAS is to look at
the LINPACK benchmark.

814
00:43:40.486 --> 00:43:42.176
So as I mentioned these
are industry standard.

815
00:43:42.176 --> 00:43:44.476
They've been around a
long time, and people came

816
00:43:44.476 --> 00:43:46.556
up with LINPACK benchmark
to see how they're doing.

817
00:43:48.586 --> 00:43:51.676
LINPACK benchmark is essentially
answering the question,

818
00:43:51.676 --> 00:43:54.356
how fast can you solve a
system of linear equations?

819
00:43:55.786 --> 00:43:57.956
There's a couple variations
of the LINPACK benchmark.

820
00:43:58.736 --> 00:44:01.436
The one that we're going to
look at here is using a matrix

821
00:43:58.736 --> 00:44:01.436
The one that we're going to
look at here is using a matrix

822
00:44:01.436 --> 00:44:03.466
of 1,000 x 1,000 elements.

823
00:44:03.846 --> 00:44:06.846
Let's look at the performance.

824
00:44:07.386 --> 00:44:13.106
So this is the LINPACK
performance of Brand A.

825
00:44:13.486 --> 00:44:15.646
Two years ago we
did this comparison

826
00:44:15.726 --> 00:44:17.206
and we compared Brand A.

827
00:44:17.846 --> 00:44:20.416
We looked around at all
the published benchmarks

828
00:44:20.416 --> 00:44:23.216
that we could find, and
they were at 40 megaflops.

829
00:44:23.736 --> 00:44:27.466
In 2 years, there's
been a lot of time,

830
00:44:27.536 --> 00:44:31.176
improvements have been
made, and that performance

831
00:44:31.176 --> 00:44:35.316
for Brand A has come
up to 788 megaflops,

832
00:44:35.796 --> 00:44:39.236
just under a gigaflop
-- pretty good.

833
00:44:39.236 --> 00:44:42.006
Let's look at the performance

834
00:44:42.006 --> 00:44:44.606
of the LINPACK benchmark using
the Accelerate Framework.

835
00:44:49.036 --> 00:44:52.856
1200 megaflops --
this is 1.2 gigaflops.

836
00:44:53.516 --> 00:44:54.266
This is pretty good.

837
00:44:55.506 --> 00:44:56.486
There's just one thing.

838
00:44:57.926 --> 00:44:59.066
We've had 2 years, too.

839
00:44:59.686 --> 00:45:04.376
This is the performance
running on the iPhone 4S.

840
00:44:59.686 --> 00:45:04.376
This is the performance
running on the iPhone 4S.

841
00:45:04.376 --> 00:45:07.216
Let's look at the performance

842
00:45:07.216 --> 00:45:19.096
of the Accelerate Framework
running on the iPhone 5.

843
00:45:19.096 --> 00:45:20.386
It's quite a bit better.

844
00:45:21.756 --> 00:45:22.156
Thank you.

845
00:45:25.076 --> 00:45:27.736
Well, LINPACK benchmark using
the Accelerate Framework

846
00:45:27.736 --> 00:45:31.326
on the iPhone 5 is
at 3,400 megaflops.

847
00:45:31.846 --> 00:45:33.896
That's 3.4 gigaflops.

848
00:45:34.516 --> 00:45:36.736
This is a phone that
fits in your pocket

849
00:45:36.776 --> 00:45:37.906
and runs on a battery.

850
00:45:38.416 --> 00:45:39.626
This is really impressive.

851
00:45:40.126 --> 00:45:44.306
As I said, the LINPACK
benchmark's been

852
00:45:44.306 --> 00:45:47.436
around for awhile, and so
we wanted to do a comparison

853
00:45:47.436 --> 00:45:48.766
to an older machine for fun.

854
00:45:49.636 --> 00:45:51.396
And so we're going
to compare the iPad

855
00:45:51.426 --> 00:45:54.156
with the Retina display
to a Power Mac G5.

856
00:45:54.156 --> 00:45:58.856
For those of you that have
been around for awhile,

857
00:45:58.856 --> 00:46:01.726
you might remember some of the
bake-offs with the Power Mac G5,

858
00:45:58.856 --> 00:46:01.726
you might remember some of the
bake-offs with the Power Mac G5,

859
00:46:02.326 --> 00:46:04.916
so we're having a
triumphant return.

860
00:46:05.926 --> 00:46:07.426
This is a 10-year old machine,

861
00:46:08.336 --> 00:46:10.666
and if any of you remember
this machine, it's returning

862
00:46:10.666 --> 00:46:11.996
with all fans blazing.

863
00:46:12.636 --> 00:46:15.176
I think there's 7 case
fans, when you turn it

864
00:46:15.176 --> 00:46:17.156
on you know it's in the room.

865
00:46:17.626 --> 00:46:20.166
When you run LINPACK benchmark,
sounds like you're driving

866
00:46:20.166 --> 00:46:21.826
down the highway with
your head out the window.

867
00:46:22.026 --> 00:46:25.046
Let's look at the performance.

868
00:46:27.166 --> 00:46:33.116
LINPACK benchmark on Power
Mac G5 is 3,643 megaflops.

869
00:46:34.406 --> 00:46:36.106
Let's see how the iPad compares.

870
00:46:38.606 --> 00:46:42.806
Just edges it out at
3,686 megaflops --

871
00:46:43.316 --> 00:46:45.446
pretty impressive
for a little tablet.

872
00:46:48.116 --> 00:46:48.816
Thank you.

873
00:46:53.306 --> 00:46:56.156
Let's look at an example
of how to use a LAPACK.

874
00:46:56.656 --> 00:46:58.236
As always, we'll begin

875
00:46:58.236 --> 00:47:00.466
by including the
Accelerate Framework header,

876
00:46:58.236 --> 00:47:00.466
by including the
Accelerate Framework header,

877
00:47:00.466 --> 00:47:03.886
and then we're going to
create and prepare our data,

878
00:47:04.426 --> 00:47:07.676
so we'll create 2 major Cs, A
and B, which describe our system

879
00:47:07.676 --> 00:47:08.446
that we want to solve.

880
00:47:09.886 --> 00:47:13.216
In this case, we're going to
use a system solve that's going

881
00:47:13.216 --> 00:47:16.446
to perform pivoting, so we need
a vector to contain information

882
00:47:16.446 --> 00:47:17.956
about the pivots that
we're going to perform,

883
00:47:18.856 --> 00:47:22.286
and then we're going to
perform this all with DGESV.

884
00:47:23.536 --> 00:47:25.146
There's a couple things
I want to point out.

885
00:47:25.956 --> 00:47:28.556
So as I mentioned, the
LAPAC is industry standard,

886
00:47:28.556 --> 00:47:30.736
it's been around for awhile.

887
00:47:30.996 --> 00:47:33.056
It's originally written
in FORTRAN and maintained

888
00:47:33.056 --> 00:47:35.546
in FORTRAN, so the entry
points look like this.

889
00:47:35.546 --> 00:47:38.496
It's going to be DGSB
followed by an underbar.

890
00:47:39.506 --> 00:47:41.636
It also means that all the
values are going to be passed

891
00:47:41.636 --> 00:47:44.156
by reference, must
something to be aware of.

892
00:47:44.156 --> 00:47:47.086
It's pretty easy to get
tripped up with this.

893
00:47:47.296 --> 00:47:50.916
But to perform the system solve,
we simply pass in the size

894
00:47:50.916 --> 00:47:53.976
of the matrix in N, the
number of right-hand sides

895
00:47:53.976 --> 00:47:55.906
which is the number of systems
that we're going to solve,

896
00:47:57.156 --> 00:47:59.446
the matrix, the leading
dimension of the matrix,

897
00:47:59.946 --> 00:48:03.046
and then the pivot
vector that we created,

898
00:47:59.946 --> 00:48:03.046
and then the pivot
vector that we created,

899
00:48:03.956 --> 00:48:05.446
and that right-hand sides B.

900
00:48:06.616 --> 00:48:10.206
Info will capture any errors
that happen in this operation.

901
00:48:10.746 --> 00:48:12.346
It's pretty easy
to solve a system

902
00:48:12.346 --> 00:48:16.816
with linear equations
with a LAPACK.

903
00:48:17.036 --> 00:48:18.066
Next is BLAS.

904
00:48:18.376 --> 00:48:23.186
So a LAPACK is the higher level
linear algebra operations.

905
00:48:23.266 --> 00:48:24.986
It's built heavily on BLAS,

906
00:48:25.016 --> 00:48:27.046
the lower level linear
algebra operations.

907
00:48:27.806 --> 00:48:30.186
All of BLAS is available through
the Accelerate Framework.

908
00:48:31.216 --> 00:48:34.216
It's typically broken down
into three categories:

909
00:48:34.666 --> 00:48:38.396
vector operations -- this is
DOT product, scalar product,

910
00:48:38.396 --> 00:48:41.796
vector sums, matrix
vector operations,

911
00:48:41.996 --> 00:48:44.986
matrix vector product,
outer product,

912
00:48:45.516 --> 00:48:49.236
and matrix/matrix operations,
like matrix multiply.

913
00:48:49.906 --> 00:48:53.766
Let's look at an example
of how to use BLAS

914
00:48:53.766 --> 00:48:54.826
in the Accelerate Framework.

915
00:48:55.336 --> 00:48:59.016
We'll begin by including the
Accelerate Framework header.

916
00:48:59.876 --> 00:49:02.296
As always we'll create
and prepare our data,

917
00:48:59.876 --> 00:49:02.296
As always we'll create
and prepare our data,

918
00:49:02.336 --> 00:49:04.766
so we'll align these
buffers if we can.

919
00:49:05.296 --> 00:49:09.786
In this case we have 2
operands matrices A and B,

920
00:49:09.786 --> 00:49:11.376
and the result matrix C.

921
00:49:15.516 --> 00:49:20.636
And then we're going to
call into C BLAS DGEM.

922
00:49:20.636 --> 00:49:22.846
BLAS supports both
row and call major,

923
00:49:22.846 --> 00:49:24.906
so the first argument is
going to be to specify

924
00:49:24.906 --> 00:49:26.206
if we're a row or call major.

925
00:49:27.236 --> 00:49:30.756
The next 2 arguments specify if
we want to perform a transpose

926
00:49:30.756 --> 00:49:32.216
on the 2 operand matrices.

927
00:49:32.736 --> 00:49:36.166
It's important with BLAS
and a LAPACK to understand

928
00:49:36.166 --> 00:49:38.676
that these transposes
don't actually happen;

929
00:49:39.046 --> 00:49:40.686
the operation is
organized as such

930
00:49:40.716 --> 00:49:43.836
that they are implied
as transposes.

931
00:49:45.016 --> 00:49:48.376
And then the last
several parameters

932
00:49:48.376 --> 00:49:50.376
for this argument are
information about the size

933
00:49:50.376 --> 00:49:52.306
of the matrix, the
matrices themselves,

934
00:49:52.306 --> 00:49:56.216
their leading dimensions,
and any scalar values

935
00:49:56.216 --> 00:49:58.666
which will scale the
operands or a result matrix.

936
00:50:02.876 --> 00:50:06.006
Just to cover some of the data
types and details supported

937
00:50:06.006 --> 00:50:08.756
by both BLAS and LAPACK,
they both support single

938
00:50:08.756 --> 00:50:13.226
and double precision values,
both real and complex,

939
00:50:13.766 --> 00:50:18.016
and multiple data
formats for your matrices,

940
00:50:18.016 --> 00:50:21.726
so dense matrices, band in
matrices, triangular matrices.

941
00:50:21.726 --> 00:50:26.226
As we saw before, transposes as
well as conjugate transposes --

942
00:50:26.446 --> 00:50:30.526
and again, these
disappear in the operation.

943
00:50:30.526 --> 00:50:32.126
They aren't explicit transposes.

944
00:50:33.086 --> 00:50:35.216
And then finally,
BLAS supports both row

945
00:50:35.216 --> 00:50:38.766
and column major while LAPACK
only supports column major.

946
00:50:41.596 --> 00:50:43.286
Another tweet I wanted
to share with you,

947
00:50:43.286 --> 00:50:46.006
playing with the Accelerate
Framework today, having BLAST.

948
00:50:48.736 --> 00:50:52.666
So in summary, there's
a lot of functionality

949
00:50:52.666 --> 00:50:53.716
in the Accelerate Framework.

950
00:50:54.476 --> 00:50:56.596
You'll find image
processing in vImage,

951
00:50:57.796 --> 00:51:01.526
digital signal processing
in vDSP,

952
00:50:57.796 --> 00:51:01.526
digital signal processing
in vDSP,

953
00:51:01.526 --> 00:51:04.876
transcendental math functions
in vForce and vMathLib

954
00:51:05.886 --> 00:51:08.626
and linear algebra,
LAPACK and BLAS.

955
00:51:09.126 --> 00:51:13.066
When you think Accelerate
Framework, think easy access

956
00:51:13.066 --> 00:51:16.016
to all this functionality,
over 2,000 APIs.

957
00:51:16.806 --> 00:51:19.556
Accurate, we tested so
that you don't have to.

958
00:51:19.556 --> 00:51:22.686
You're going to get
great performance

959
00:51:22.966 --> 00:51:24.166
with low energy usage.

960
00:51:24.166 --> 00:51:30.146
It's going to work great on OS X
and iOS, and it's going to work

961
00:51:30.146 --> 00:51:32.076
on the complete Apple
hardware lineup,

962
00:51:32.726 --> 00:51:35.486
everything that's available now
and everything that's to come.

963
00:51:36.016 --> 00:51:40.616
Just a recap of the
tips to be successful

964
00:51:40.616 --> 00:51:41.746
with the Accelerate Framework.

965
00:51:42.516 --> 00:51:43.756
When you're preparing your data,

966
00:51:43.876 --> 00:51:45.926
if you can make the
buffers contiguous

967
00:51:46.186 --> 00:51:48.006
and you can align the
beginning of those buffers

968
00:51:48.006 --> 00:51:50.116
to a 16-byte boundary, we can

969
00:51:50.116 --> 00:51:52.466
in some cases get you
slightly more performance.

970
00:51:53.136 --> 00:51:55.306
Again, Accelerate
Framework is always going

971
00:51:55.306 --> 00:51:57.136
to give you the best
performance possible

972
00:51:57.136 --> 00:51:58.806
when you can't meet
these recommendations.

973
00:51:59.706 --> 00:52:01.246
Understand the problem size.

974
00:51:59.706 --> 00:52:01.246
Understand the problem size.

975
00:52:01.846 --> 00:52:04.436
For small problem sets,

976
00:52:04.436 --> 00:52:06.146
the Accelerate Framework
might not be able

977
00:52:06.146 --> 00:52:07.756
to deliver the best performance.

978
00:52:08.086 --> 00:52:11.106
It's always going to deliver
the functionality, though.

979
00:52:11.966 --> 00:52:13.866
Finally, do set up
and destroy once.

980
00:52:14.016 --> 00:52:17.436
If you find yourself
creating a setup structure,

981
00:52:17.466 --> 00:52:19.836
use that setup structure
as many times as possible.

982
00:52:20.476 --> 00:52:25.536
The Accelerate Framework is
for you guys, and so I want

983
00:52:25.536 --> 00:52:26.186
to leave you with this.

984
00:52:26.186 --> 00:52:28.776
If you need a feature,
please request it.

985
00:52:29.236 --> 00:52:33.196
The best way to do that
is by filing a bug.

986
00:52:33.416 --> 00:52:34.626
And one more tweet:

987
00:52:34.936 --> 00:52:37.656
"The discrete cosine transform
was my feature request

988
00:52:37.656 --> 00:52:39.226
that made it into the
Accelerate Framework.

989
00:52:39.276 --> 00:52:40.276
I feel so special."

990
00:52:41.006 --> 00:52:42.236
So we do listen.

991
00:52:43.016 --> 00:52:43.686
Please request.

992
00:52:44.256 --> 00:52:46.176
And then lastly, thanks, Apple,

993
00:52:46.176 --> 00:52:47.586
for making the Accelerate
Framework.

994
00:52:48.096 --> 00:52:49.866
Thank you, guys, for
making it a success.

995
00:52:50.516 --> 00:52:54.996
[Applause]

996
00:52:55.496 --> 00:52:58.146
Just a little more information
here, if you guys need to get

997
00:52:58.146 --> 00:53:00.736
in touch with us,
contact Paul or George.

998
00:52:58.146 --> 00:53:00.736
in touch with us,
contact Paul or George.

999
00:53:01.116 --> 00:53:04.446
There's some documentation
available online, and as always,

1000
00:53:04.476 --> 00:53:06.186
check the Apple developer
forums.

1001
00:53:06.806 --> 00:53:08.756
That's all we got,
thank you, guys.

1002
00:53:10.516 --> 00:53:18.270
[Silence]
